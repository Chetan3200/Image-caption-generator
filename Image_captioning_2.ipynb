{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e38dd76-85ae-45ce-989f-a2affc0eec91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to navigate through directories\n",
    "import os \n",
    "import pickle\n",
    "import string \n",
    "import tensorflow\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import add\n",
    "from keras.models import Model,load_model\n",
    "# ModelCheckpoint is a class that is used to save weights of a model during training process\n",
    "from keras.callbacks import ModelCheckpoint \n",
    "# Tokenizer class is used to preprocess text data and convert text corpus into integer tokens\n",
    "# Which can then be passed to a NN for training\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "# to_categorical converts class vectors into class matrices\n",
    "from keras.utils import to_categorical,plot_model\n",
    "# plot_sequences pads sequences of integer tokens to the same length\n",
    "# often used with text data where length of sequences may vary\n",
    "# This function is used after \"Tokenizer\" class\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.applications.vgg16 import VGG16,preprocess_input\n",
    "from keras.layers import Input,Dense,LSTM,Embedding,Dropout\n",
    "# load_img loads an image from a file and returns it as a Numpy array, size of \n",
    "# this nummpy array is specified as target_size\n",
    "# img_to_array converts image represented as a NumPy array to a single-channel \n",
    "# image array with dimensions\n",
    "from keras_preprocessing.image import img_to_array,load_img\n",
    "from nltk.translate.bleu_score import sentence_bleu,corpus_bleu\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a3d4b0-9caf-4444-b77e-0bb478c16a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function that takes in a directory as an argument and returns a dictionary\n",
    "of features extracted from the images in the directory\n",
    "'''\n",
    "def extract_features(directory):\n",
    "  # creating a VGG16 model and removing its final layer by .pop()\n",
    "  model = VGG16()\n",
    "  model.layers.pop()\n",
    "  # creating a new model with inputs and outputs of the modified VGG16 model\n",
    "  # for the input argument we pass the input tensors of original VGG16 \n",
    "  # for output we pass the out tensor of the last layer of original VGG16\n",
    "  # this way we get a new model which can be used as a feature extractor\n",
    "  # as it will only output features from last layer instead os full set of prediction\n",
    "  model = Model(inputs=model.inputs,outputs=model.layers[-1].output)\n",
    "  print(model.summary())\n",
    "  features = {}\n",
    "  i=0\n",
    "  for name in os.listdir(directory):\n",
    "    print(i)\n",
    "    img = load_img(directory+'/'+name,target_size=(224,224))\n",
    "    img = img_to_array(img)\n",
    "    img = img.reshape((1,img.shape[0],img.shape[1],img.shape[2]))\n",
    "    img = preprocess_input(img)\n",
    "    feature = model.predict(img,verbose=0)\n",
    "    img_id = name.split('.')[0]\n",
    "    features[img_id] = feature\n",
    "    i+=1\n",
    "  return features\n",
    "directory ='drive/My Drive/image_captioning/Flicker8k/Flicker8k_Dataset'\n",
    "features = extract_features(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049ee602-e9fd-4cc2-8955-ef474ed9d98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "load_description takes as input a filename, opens it is read mode and reading\n",
    "lines of the file using readlines. Then we split each line into tokens.\n",
    "Ignore the token if it's shorter than 2 tokens. First element of token is \n",
    "image_id, rest of it is image description, which we join together.\n",
    "mappings is a dictionary, image_id -> image_desc. We return mappins dict.\n",
    "'''\n",
    "def load_description(filename):\n",
    "  mappings = {}\n",
    "  file = open(filename,'r')\n",
    "  content = file.readlines()\n",
    "  file.close()\n",
    "  for lines in content:\n",
    "    tokens = lines.split()\n",
    "    if len(lines)<2:\n",
    "      continue\n",
    "    image_id,image_desc = tokens[0].split('.')[0],tokens[1:]\n",
    "    image_desc = ' '.join(image_desc)\n",
    "    if image_id not in mappings:\n",
    "      mappings[image_id] = []\n",
    "    mappings[image_id].append(image_desc)\n",
    "  return mappings\n",
    "\n",
    "'''\n",
    "takes in a dictionary of descriptions as argument and cleans them by removing \n",
    "punctuations and making all of the words lowercase.\n",
    "'''\n",
    "def clean_description(descriptions):\n",
    "  table = str.maketrans('','',string.punctuation)\n",
    "  for k,image_descriptions in descriptions.items():\n",
    "    for i in range(len(image_descriptions)):\n",
    "      desc = image_descriptions[i]\n",
    "      desc = desc.split()\n",
    "      desc = [x.lower() for x in desc]\n",
    "      desc = [w.translate(table) for w in desc]\n",
    "      desc = [x for x in desc if len(x)>1]\n",
    "      desc = [x for x in desc if x.isalpha()]\n",
    "      image_descriptions[i] = ' '.join(desc)\n",
    "    \n",
    "'''\n",
    "Takes in a dictionary of description and returns a set containing\n",
    "all of the unique words in the descriptions. For each key, the function\n",
    "interates over descriptions and splits each one into a lists of words.\n",
    "Then we update the corpus set with the words in each description.\n",
    ".update() method with sets only updates unique words, so there will be no \n",
    "duplicates in corpus.\n",
    "'''\n",
    "def create_corpus(descriptions):\n",
    "  corpus = set()\n",
    "  for k in descriptions.keys():\n",
    "        # for x in descriptions[k]:\n",
    "        #     corpus.update(x.split())\n",
    "    [corpus.update(x.split()) for x in descriptions[k]]\n",
    "  return corpus\n",
    "\n",
    "\n",
    "'''\n",
    "writes descriptions to the file specified\n",
    "'''\n",
    "def save_descriptions(desc,filename):\n",
    "  lines = []\n",
    "  for k,v in desc.items():\n",
    "    for description in v:\n",
    "      lines.append(k+' '+description)\n",
    "  data = '\\n'.join(lines)\n",
    "  file = open(filename,'w')\n",
    "  file.write(data)\n",
    "  file.close()\n",
    "# load all descriptions\n",
    "filename = 'drive/My Drive/image_captioning/Flicker8k/Flickr8k.token.txt'\n",
    "descriptions = load_description(filename)\n",
    "print('Descriptions loaded: ',len(descriptions))\n",
    "# clean the loaded descriptions\n",
    "clean_description(descriptions)\n",
    "# check the vocabulary length\n",
    "vocabulary = create_corpus(descriptions)\n",
    "print('Vocabulary length: ',len(vocabulary))\n",
    "save_descriptions(descriptions,'drive/My Drive/image_captioning/descriptions.txt')\n",
    "print('SAVED !!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95e4db5-dfc4-496e-adf3-972380ab1173",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "function to obtain the set of image ids\n",
    "'''\n",
    "def load_set_of_image_ids(filename):\n",
    "  file = open(filename,'r')\n",
    "  lines = file.readlines()\n",
    "  file.close()\n",
    "  image_ids = set()\n",
    "  for line in lines:\n",
    "    if len(line)<1:\n",
    "      continue\n",
    "    image_ids.add(line.split('.')[0])\n",
    "  return image_ids\n",
    "\n",
    "\n",
    "'''\n",
    "Takes in two filenames and returns a dictionary of image desc.\n",
    "first split each line in the file by space character, then split the\n",
    "first character of the token list on '.'. First element of this split\n",
    "is image_id and remaining is called image_desc.\n",
    "if image_id is present in train_desc_names set, the function adds the\n",
    "image_id and image_desc to descriptions dictionary. If not present,\n",
    "the function does nothing.\n",
    "'''\n",
    "def load_clean_descriptions(all_desc,train_desc_names):\n",
    "  file = open(all_desc,'r')\n",
    "  lines = file.readlines()\n",
    "  descriptions = {}\n",
    "  for line in lines:\n",
    "    tokens = line.split()\n",
    "    image_id,image_desc = tokens[0].split('.')[0],tokens[1:]\n",
    "    if image_id in train_desc_names:\n",
    "      if image_id not in descriptions:\n",
    "        descriptions[image_id] = []\n",
    "      desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "      descriptions[image_id].append(desc)\n",
    "  return descriptions\n",
    "\n",
    "'''\n",
    "takes in a filename and dataset and returns a dictionary of image features\n",
    "using pickle to open the specified file in binary read mode and load contents\n",
    "of the file into the variable \"all_features\". Create a new dictionary and \n",
    "iterate over keys in dataset and for each key, add an entry to the \n",
    "\"features\" dictionary with key k and the value \"all_features[k]\"\n",
    "'''\n",
    "def load_image_features(filename,dataset):\n",
    "  all_features = pickle.load(open(filename,'rb'))\n",
    "  features = {k:all_features[k] for k in dataset}\n",
    "  return features\n",
    "\n",
    "# load train image ids\n",
    "train = 'drive/My Drive/image_captioning/Flicker8k/Flickr_8k.trainImages.txt'\n",
    "train_image_ids = load_set_of_image_ids(train)\n",
    "print('Training images found: ',len(train_image_ids))\n",
    "\n",
    "# load training descriptions\n",
    "train_descriptions = load_clean_descriptions('drive/My Drive/image_captioning/descriptions.txt',train_image_ids)\n",
    "print('training descriptions loaded: ',len(train_descriptions))\n",
    "\n",
    "# load training image features\n",
    "train_features = load_image_features('drive/My Drive/image_captioning/Flicker_dataset_image_features.pkl',train_image_ids)\n",
    "print('training features loaded: ',len(train_features))\n",
    "\n",
    "train_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8524752-b69c-4009-a254-ea40e7e895c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "converts a dictionary called descriptions into a list of\n",
    "all the descriptions.\n",
    "'''\n",
    "def to_list(descriptions):\n",
    "  all_desc_list = []\n",
    "  for k,v in descriptions.items():\n",
    "    for desc in v:\n",
    "      all_desc_list.append(desc)\n",
    "  return all_desc_list\n",
    "\n",
    "\n",
    "'''\n",
    "first we convert the discriptions dictionary into a list.\n",
    "Then we create a 'Tokenizer' object and call the 'fit_on_texts'\n",
    "method, passing the list as argument. \n",
    "Tokenizer then creates a mapping from words to integers.\n",
    "'''\n",
    "def tokenization(descriptions):\n",
    "  # list of all the descriptions\n",
    "  all_desc_list = to_list(descriptions)  \n",
    "  tokenizer = Tokenizer()\n",
    "  tokenizer.fit_on_texts(all_desc_list)\n",
    "  return tokenizer\n",
    "\n",
    "# create tokenizer\n",
    "tokenizer = tokenization(train_descriptions)\n",
    "\n",
    "# word index is the dictionary /mappings of word-->integer\n",
    "# word_index maps words to integers. Integers are assigned in \n",
    "# such a way that the most frequently occuring word have lowest\n",
    "# integer values. +1 cuz tokenizer object assignes integer value \n",
    "# of 0 to words that are not in vocabulory.\n",
    "vocab_size = len(tokenizer.word_index)+1\n",
    "print('Vocab size: ',vocab_size)\n",
    "\n",
    "def max_length(descriptions):\n",
    "  all_desc_list = to_list(descriptions)\n",
    "  return (max(len(x.split()) for x in all_desc_list))\n",
    "\n",
    "\n",
    "def create_sequences(tokenizer,desc_list,max_len,photo):\n",
    "  X1,X2,y = [],[],[]\n",
    "  # X1 will contain photo\n",
    "  # X2 will contain current sequence\n",
    "  # y will contain one hot encoded next word\n",
    "\n",
    "  for desc in desc_list:\n",
    "    # tokenize descriptions\n",
    "    seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "    for i in range(1,len(seq)):\n",
    "      # out seq is basically the next word in the sentence\n",
    "      in_seq,out_seq = seq[:i],seq[i]\n",
    "      # pad input sequence\n",
    "      in_seq = pad_sequences([in_seq],maxlen=max_len)[0]\n",
    "      # one hot encode output sequence\n",
    "      out_seq = to_categorical([out_seq],num_classes=vocab_size)[0]\n",
    "      X1.append(photo)\n",
    "      X2.append(in_seq)\n",
    "      y.append(out_seq)\n",
    "  return np.array(X1),np.array(X2),np.array(y)\n",
    "\n",
    "# maximum length that a description can have OR the biggest description we are having\n",
    "max_len = max_length(train_descriptions)\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8415539-ea83-4652-9709-8f27855b256b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(descriptions,photos,tokenizer,max_len):\n",
    "  while 1:\n",
    "    for k,desc_list in descriptions.items():\n",
    "      photo = photos[k][0]\n",
    "      in_img,in_seq,out_seq = create_sequences(tokenizer,desc_list,max_len,photo)\n",
    "      yield[[in_img,in_seq],out_seq]\n",
    "\n",
    "def define_model(vocab_size, max_length):\n",
    "    # image features extractor model\n",
    "    inputs1 = Input(shape=(4096,))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    " \n",
    "    # input sequence model\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "     # embedding(input_dimension,output_dimension,)\n",
    "     # input dim is always the vocabulary size \n",
    "    # output dimension tells the size of vector space in which the words will be embedded\n",
    "    # mask zero is used when the input itself is 0 then to not confuse it with padded zeros it is used as True\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "\n",
    "    # decoder model OR output word model\n",
    "    decoder1 = add([fe2, se3])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "    # summarize model\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e72eba-9136-4157-9c26-008ec876f4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_model(vocab_size,max_len)\n",
    "epochs = 20\n",
    "steps = len(train_descriptions)\n",
    "for i in range(epochs):\n",
    "  generator = data_generator(train_descriptions,train_features,tokenizer,max_len)\n",
    "  model.fit_generator(generator,epochs=1,steps_per_epoch=steps,verbose=1)\n",
    "  model.save('drive/My Drive/image_captioning/model_'+str(i)+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c1a8e9-b8d9-4e35-a0cc-461be9e6be79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def int2word(tokenizer,integer):\n",
    "  for word,index in tokenizer.word_index.items():\n",
    "    if index==integer:\n",
    "      return word\n",
    "  return None\n",
    "\n",
    "def predict_desc(model,tokenizer,photo,max_len):\n",
    "  in_seq = 'startseq'\n",
    "  for i in range(max_len):\n",
    "    seq = tokenizer.texts_to_sequences([in_seq])[0]\n",
    "    seq = pad_sequences([seq],maxlen=max_len)\n",
    "    y_hat = model.predict([photo,seq],verbose=0)\n",
    "    y_hat = np.argmax(y_hat)\n",
    "    word = int2word(tokenizer,y_hat)\n",
    "    if word==None:\n",
    "      break\n",
    "    in_seq = in_seq+' '+word\n",
    "    if word=='endseq':\n",
    "      break\n",
    "  return in_seq\n",
    "\n",
    "def evaluate_model(model,descriptions,photos,tokenizer,max_len):\n",
    "  actual,predicted = [],[]\n",
    "  for key,desc in descriptions.items():\n",
    "    y_hat = predict_desc(model,tokenizer,photos[key],max_len)\n",
    "    references = [d.split() for d in desc]\n",
    "    actual.append(references)\n",
    "    predicted.append(y_hat.split())\n",
    "  print('BLEU-1: %f' %corpus_bleu(actual,predicted,weights=(1.0,0,0,0)))\n",
    "  print('BLEU-2: %f' %corpus_bleu(actual,predicted,weights=(0.5,0.5,0,0)))\n",
    "  print('BLEU-3: %f' %corpus_bleu(actual,predicted,weights=(0.33,0.33,0.33,0)))\n",
    "  print('BLEU-4: %f' %corpus_bleu(actual,predicted,weights=(0.25,0.25,0.25,0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fde2103-6f83-4621-bbcb-aee0ea7f6ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################  load training data (6k)  ##########################\n",
    "train = 'drive/My Drive/image_captioning/Flicker8k/Flickr_8k.trainImages.txt'\n",
    "train_image_ids = load_set_of_image_ids(train)\n",
    "print('Training images found: ',len(train_image_ids))\n",
    "\n",
    "# load training descriptions\n",
    "train_descriptions = load_clean_descriptions('drive/My Drive/image_captioning/descriptions.txt',train_image_ids)\n",
    "print('training descriptions loaded: ',len(train_descriptions))\n",
    "\n",
    "tokenizer = tokenization(train_descriptions)\n",
    "max_len = max_length(train_descriptions)\n",
    "\n",
    "####################  load test data  ##########################\n",
    "test = 'drive/My Drive/image_captioning/Flicker8k/Flickr_8k.testImages.txt'\n",
    "test_image_ids = load_set_of_image_ids(test)\n",
    "print('Test images found: ',len(test_image_ids))\n",
    "\n",
    "# load test descriptions\n",
    "test_descriptions = load_clean_descriptions('drive/My Drive/image_captioning/descriptions.txt',test_image_ids)\n",
    "print('test descriptions loaded: ',len(test_descriptions))\n",
    "\n",
    "# load test image features\n",
    "test_features = load_image_features('drive/My Drive/image_captioning/Flicker_dataset_image_features.pkl',test_image_ids)\n",
    "print('training features loaded: ',len(test_features))\n",
    "#################################################################\n",
    "filename = 'drive/My Drive/image_captioning/model_18.h5'\n",
    "model = load_model(filename)\n",
    "evaluate_model(model,test_descriptions,test_features,tokenizer,max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da651dc-bb3f-4844-aef1-955b0e5a3150",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_to_test = 'drive/My Drive/image_captioning/983801190.jpg'\n",
    "img = plt.imread(img_to_test)\n",
    "plt.imshow(img)\n",
    "\n",
    "def extract_features(filename):\n",
    "    # load the model\n",
    "    model = VGG16()\n",
    "    # re-structure the model\n",
    "    model.layers.pop()\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "    # load the photo\n",
    "    image = load_img(filename, target_size=(224, 224))\n",
    "    # convert the image pixels to a numpy array\n",
    "    image = img_to_array(image)\n",
    "    # reshape data for the model\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    # prepare the image for the VGG model\n",
    "    image = preprocess_input(image)\n",
    "    # get features\n",
    "    feature = model.predict(image, verbose=0)\n",
    "    return feature\n",
    "\n",
    "# pre-define the max sequence length (from training)\n",
    "max_length = 34\n",
    "# load the model\n",
    "model = load_model('drive/My Drive/image_captioning/model_18.h5')\n",
    "# load and prepare the photograph\n",
    "photo = extract_features(img_to_test)\n",
    "# generate description\n",
    "description = predict_desc(model, tokenizer, photo, max_length)\n",
    "\n",
    "description = ' '.join(description.split()[1:-1])\n",
    "print()\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3555f195-8722-4964-a327-51bedeb38536",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9958ab-43c8-4b7a-ba4b-610965bfd5f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b6bcd4-f66b-4823-be65-195c8b5cb98a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
